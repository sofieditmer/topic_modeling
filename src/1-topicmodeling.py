#!/usr/bin/env python
"""
Info: This script performs topic modeling on the clean tweets by Donald Trump. The number of topics is estimated by computing coherence values for different number of topics, and an LDA model is constructed with the number of topics with the highest coherence value. Visualizations of the topics are created relying on pyLDAvis and wordcloud and these visualizations are saved in the output directory. 

Parameters:
    (optional) input_file: str <name-of-input-file>, default = clean_trump_tweets.csv
    (optional) chunk_size: int <size-of-chunks>, default = 10
    (optional) passes: int <number-of-passes>, default = 10
    (optional) min_count: int <minimum-count-bigrams>, default = 2
    (optional) threshold: int <threshold-for-keeping-phrases>, default = 100
    (optional) iterations: int <number-of-iterations>, default = 100
    (optional) rolling_mean: int <rolling-mean>, default = 50
    (optional) step_size: int <size-of-steps>, default = 5

Usage:
    $ python 1-topicmodeling.py

Output: 
    - topics.txt: overview of topics generated by the LDA model
    - dominant_topic.csv: table showing the most dominant topics and their associated keywords as well as how much each topic contributes. 
    - topic_contributions.csv: a dataframe showing the most contributing keywords for each topic.
    - topics_over_time.jpg: visualization of the topic contributions over time.
    - topic_wordclouds.png: the topics visualized as word clouds.
"""

### DEPENDENCIES ###

# core libraries
import sys
import os
sys.path.append(os.path.join(".."))

# numpy, pandas, pyplot
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

# spaCy
import spacy
nlp = spacy.load("en_core_web_sm", disable=["ner"])
nlp.max_length = 68000000 # increasing maximum length

# pyLDAvis and seaborn for vizualisations
import pyLDAvis.gensim
import seaborn as sns

# matplotlib colors
import matplotlib.colors as mcolors

# wordcloud tools
from wordcloud import WordCloud

# LDA tools
import gensim
import gensim.corpora as corpora
from gensim.models import CoherenceModel
from utils import lda_utils

# Ignore warnings
import logging, warnings
warnings.filterwarnings('ignore')
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)

# argparse
import argparse


### MAIN FUNCTION ###

def main():
    
    ### ARGPARSE ###
    
    # Initialize ArgumentParser class
    ap = argparse.ArgumentParser()
    
    # Argument 1: Input file
    ap.add_argument("-i", "--input_filename",
                    type = str,
                    required = False, # not required argument
                    help = "Name of input file",
                    default = "clean_trump_tweets.csv") # default
 
    # Argument 2: Number of passes
    ap.add_argument("-p", "--n_passes",
                    type = int,
                    required = False, # not required argument
                    help = "Define the number of passes which is the number of times you want the model to go through the entire corpus.",
                    default = 10) # default number of passes
    
    # Argument 3: Minimum count for bigrams
    ap.add_argument("-m", "--min_count",
                    type = int,
                    required = False, # not required argument
                    help = "Define the minimum count for bigrams to occur to be included",
                    default = 2) # default minimum count
    
    # Argument 4: Threshold
    ap.add_argument("-th", "--threshold",
                    type = int,
                    required = False, # not required argument
                    help = "Define the threshold which determines which phrases to include and which to exlude. The higher the threshold, the fewer the number of phrases are included.",
                    default = 100) # default threshold
    
    # Argument 5: Iterations
    ap.add_argument("-it", "--n_iterations",
                    type = int,
                    required = False, # not required argument
                    help = "Define the number of iterations through each document in the corpus",
                    default = 100) # default number of iterations
    
    # Argument 6: Rolling mean size
    ap.add_argument("-r", "--rolling_mean",
                    type = int,
                    required = False, # not required argument
                    help = "Define the rolling mean which is the number of chunks of tweets to calculate contribution of at a time",
                    default = 50) # default
    
    # Argument 7: Step size
    ap.add_argument("-s", "--step_size",
                    type = int,
                    required = False, # not required argument
                    help = "Define the step size",
                    default = 5) # default step size
    
    # Argument 8: Chunk size
    ap.add_argument("-c", "--chunk_size",
                    type = int,
                    required = False, # not required argument
                    help = "Define the size of the chunks, i.e. how many tweets one chunk should consist of.",
                    default = 10) # default chunk size

    # Parse arguments
    args = vars(ap.parse_args())
    
    # Save input parameters
    input_file = os.path.join("..", "data", args["input_filename"])
    n_passes = args["n_passes"]
    min_count = args["min_count"]
    threshold = args["threshold"]
    n_iterations = args["n_iterations"]
    rolling_mean = args["rolling_mean"]
    step_size = args["step_size"]
    chunk_size = args["chunk_size"]
    
    # Create output directory if it does not already exist
    if not os.path.exists(os.path.join("..", "output")):
        os.mkdir(os.path.join("..", "output"))

    # Start message to user
    print("\n[INFO] Initializing topic modeling on all Donald Trump tweets from May 2009 to June 2020...")
    
    # Instantiate the topic modeling class
    topic_modeling = Topic_modeling(input_file)
    
    # Load and prepare data
    print(f"\n[INFO] Loading '{input_file}'...")
    clean_tweets_df = topic_modeling.load_data()
    
    # Chunk data
    print(f"\n[INFO] Chunking the data into chunks of {chunk_size}...")
    chunks = topic_modeling.chunk_tweets(clean_tweets_df, chunk_size)
    
    # Process data
    print("\n[INFO] Creating bigram and trigram models and performing lemmatization and part-of-speech-tagging...")
    processed_data = topic_modeling.process_data(chunks, min_count, threshold)
    
    # Create bag of words
    print("\n[INFO] Creating dictionary and word corpus...")
    id2word, corpus = topic_modeling.create_dict_corpus(processed_data)
    
    # Estimate the optimal number of topics
    print("\n[INFO] Finding the optimal number of topics...")
    optimal_n_topics = topic_modeling.find_optimal_n_topics(processed_data, corpus, id2word, step_size)
    
    # Print the optimal number of topics to the screen
    print(f"\nThe optimal number of topics is {optimal_n_topics}")  
    
    # Create LDA model and compute perplexity and coherence scores
    print("\n[INFO] Creating LDA model...")
    lda_model, perplexity_score, coherence_score = topic_modeling.create_lda(processed_data, id2word, corpus, optimal_n_topics, chunk_size, n_passes, n_iterations)
    
    # Create outputs
    print("\n[INFO] Producing outputs and saving to 'output' directory...")
    
    # Output 1
    topic_modeling.create_output_1(lda_model, perplexity_score, coherence_score, optimal_n_topics)
    print("\n[INFO] A txt-file containing the topics has been saved to output directory...")
    
    # Output 2
    df_dominant_topic, df_topic_keywords = topic_modeling.create_output_2(lda_model, corpus, processed_data, optimal_n_topics, )
    print("\n[INFO] A dataframe showing the most dominant topic for each chunk has been saved to output directory...")
    
    # Output 3
    topic_modeling.create_output_3(df_dominant_topic, lda_model, corpus, processed_data, optimal_n_topics, df_topic_keywords)
    print("\n[INFO] A dataframe showing the most contributing keywords for each topic has been saved to output directory...")
    
    # Create visualization: topics over time with rolling mean
    print("\n[INFO] Creating visualization of topic contributions over time...")
    topic_modeling.visualize_topics(processed_data, rolling_mean, lda_model, corpus)
    
    # Create word clouds of topics
    print("\n[INFO] Creating word clouds of topics...")
    topic_modeling.create_wordcloud(lda_model, optimal_n_topics)

    # User message
    print("\n[INFO] Done! You have now performed topic modeling on all of Donald Trump tweets from May 2009 to June 2020. The results have been saved in the 'output' folder.\n")
    
    
### TOPIC MODELING ### 
    
# Creating Topic modeling class 
class Topic_modeling:
    
    # Intialize Preprocessing class
    def __init__(self, input_file):
        
        # Receive input
        self.input_file = input_file
        
    
    def load_data(self):
        """
        This method loads the preprocessed data from the data folder. 
        """
        # Load data into dataframe with pandas
        clean_tweets_df = pd.read_csv(self.input_file, lineterminator = "\n")
        
        # Take only relevant columns
        clean_tweets_df = clean_tweets_df.loc[:, ("id", "date", "clean_tweets")]
        
        # Drop rows with missing values
        clean_tweets_df = clean_tweets_df.dropna(subset=['clean_tweets'])
    
        return clean_tweets_df
    
    
    def chunk_tweets(self, clean_tweets_df, chunk_size):
        """
        This method creates chunks of tweets and saves the chunks in a new column in the dataframe. Chuking the tweets 
        as opposed to having individual tweets is performed to ensure that clear topics are found. By chunking the tweets,
        the topics become more interpretable.
        """
        # Create empty list for chunks of tweets
        chunks = []
        
        # Create chunks of tweets
        for i in range(0, len(clean_tweets_df["clean_tweets"]), chunk_size):
            chunks.append(' '.join(clean_tweets_df["clean_tweets"][i:i+chunk_size]))
            
        return chunks

    
    def process_data(self, chunks, min_count, threshold):
        """
        This method creates bigram and trigram models, and performs lemmatization and part-of-speech-tagging.
        The threshold value determines which phrases to include. The higher the threshold, the fewer phrases are included,
        because the most frequent bigrams are excluded. Removing the most frequent phrases ensures that only the most 
        semantically meaningful phrases are kept, and potential noise is filtered out.
        The bigrams are created based on the words that appear one after another most frequently, and the bigrams are then
        fed into a trigram generator which creates the trigrams based on the bigrams.
        The output of this method is a list of the nouns, verbs, and adjectives within the data.
        """
        # Create model of bigrams and trigrams 
        bigram = gensim.models.Phrases(chunks, min_count = min_count, threshold = threshold) # higher threshold fewer phrases. The min_count is the minimum number of times the bigram should occur to be included
        trigram = gensim.models.Phrases(bigram[chunks], threshold = threshold) # the trigram model is based on the bigram model
    
        # Fit the models to the data
        bigram_mod = gensim.models.phrases.Phraser(bigram)
        trigram_mod = gensim.models.phrases.Phraser(trigram)
    
        # Lemmatize and part-of-speech tag 
        processed_data = lda_utils.process_words(chunks,
                                                 nlp,
                                                 bigram_mod,
                                                 trigram_mod,
                                                 allowed_postags=["NOUN", "VERB", "ADJ"]) # nouns, verbs, and adjectives
        
        return processed_data


    def create_dict_corpus(self, processed_data):
        """
        This method creates the dictionary and corpus. In other words, it creates a representation of words within a document
        in terms of how often an indivudal word occurs in each document. Hence, the documents are conceptualized as a 
        bag-of-words model. This means that we are no longer dealing with words, but rather we with distributions of word
        frequencies (i.e., a numerical representation). 
        The dictionary is created by converting each word into an integer value. The corpus is created by converting the 
        documents to a "bag of words" model. 
        """
        # Create dictionary
        id2word = corpora.Dictionary(processed_data)
    
        # Create Ccrpus: Term Document Frequency
        corpus = [id2word.doc2bow(text) for text in processed_data]
    
        return id2word, corpus


    def find_optimal_n_topics(self, processed_data, corpus, id2word, step_size):
        """
        This method runs the model multiple times with different numbers of topics and find the optimal number based 
        on the maximum coherence value. Hence, the number of topics with the highest coherence value is chosen as the 
        most optimal number of topics. A high coherence value ensures that the topics are "coherent", i.e., meaningful.
        """
        # Run model multiple times
        model_list, coherence_values = lda_utils.compute_coherence_values(texts = processed_data,
                                                                          corpus = corpus,
                                                                          dictionary = id2word,
                                                                          start = 5,
                                                                          limit = 15,
                                                                          step = step_size)
        # Find the maximum coherence value
        max_coherence = np.argmax(coherence_values)
    
        # Find the number of topics corresponding to the maximum coherence value
        optimal_n_topics = model_list[max_coherence].num_topics
    
        return optimal_n_topics


    def create_lda(self, processed_data, id2word, corpus, optimal_n_topics, chunk_size, n_passes, n_iterations):
        """
        This method builds the LDA model using gensim's multicore function, and computes perplexity and coherence scores.
        When we are calculating perplexity we measure how well the model is performing, i.e. the amount of error. 
        Ideally, error ("surprise") should be low, because this implies that when the model encounters new data, 
        it is less "surprised". Hence, the perplexity score should be minimized. Ideally, the coherence value should be high. 
        A high coherence value means that the topics are very coherent, which means that the topics actually correspond/relate
        to something in the data. 
        """
        # Define and run LDA model
        lda_model = gensim.models.LdaMulticore(corpus=corpus,                 # vectorized corpus (list of lists of tuples)
                                               id2word=id2word,               # gensim dictionary (mapping words to IDs)
                                               num_topics=optimal_n_topics,   # number of topics
                                               random_state=100,              # random state for reproducibility
                                               chunksize=chunk_size,          # the number of chunks to process at a time. Rather than processing one chunk at a time, we process batches of 10 chunkes which is more efficient. Increasing the chunk/batch size means that the model will train quicker.  
                                               passes=n_passes,               # passes/epochs is the number of times the model should go through the entire corpus.
                                               iterations=n_iterations,       # the number of iterations is how often the model go through the single document in the corpus
                                               per_word_topics=True,          # defining word distributions for greater interpretatbility
                                               minimum_probability=0.0)       # in some cases a topic does not appear at all in any document, and I do not want to exclude these topics but rather keep them, which is why I set it to return 0 instead of nothing which is the default
    
        # Calculate perplexity score
        perplexity_score = lda_model.log_perplexity(corpus)
    
        # Calculate coherence score
        coherence_model_lda = CoherenceModel(model=lda_model,
                                             texts=processed_data,
                                             dictionary=id2word,
                                             coherence='c_v')
    
        coherence_score = coherence_model_lda.get_coherence()
          
        return lda_model, perplexity_score, coherence_score
         
    
    def create_output_1(self, lda_model, perplexity_score, coherence_score, optimal_n_topics):
        """
        This method creates a txt-file containing the topics, perplexity, and coherence scors and saves it to 
        the 'output' directory.
        """
        # Extract the topics
        topics = lda_model.print_topics()
    
        # Define path
        out_path = os.path.join("..", "output", "topics.txt")
    
        # Write txt-file containing the topics, perplexity, and coherence scores
        with open(out_path, "w+") as f:
            # Print how many topics the model has
            f.writelines(f"The model has {optimal_n_topics} topics.\n")
            # Print perplexity and coherence scores
            f.writelines(f"Perplexity score: {perplexity_score}, Coherence score: {coherence_score} \n")
            # Print topics
            f.writelines(f"\nOverview of topics: \n {topics}")
            
            
    def create_output_2(self, lda_model, corpus, processed_data, optimal_n_topics):
        """
        This method creates a dataframe showing the most dominant topic for each chunk and saves it to the 'output' directory.
        """
        # Find keywords for each topic
        df_topic_keywords = lda_utils.format_topics_sentences(ldamodel=lda_model,
                                                              corpus=corpus,
                                                              texts=processed_data)
        
        # Find the most dominant topic per chunk
        df_dominant_topic = df_topic_keywords.reset_index()
        df_dominant_topic.columns = ['Chunk_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
    
        # Save dataframe to output folder
        output_path = os.path.join("..", "output", "dominant_topic.csv")
        df_dominant_topic.to_csv(output_path, index = False)
        
        return df_dominant_topic, df_topic_keywords
    
    
    def create_output_3(self, df_dominant_topic, lda_model, corpus, processed_data, optimal_n_topics, df_topic_keywords):
        """
        This method creates a dataframe showing the most contributing keywords for each topic and saves it to 
        the 'output' directory.
        """
        # Display setting to show more characters in column
        pd.options.display.max_colwidth = 100
    
        # Create dataframe
        sentence_topics_sorted_df = pd.DataFrame()

        # Group keywords by the most dominant topic
        sentence_topics_grouped = df_topic_keywords.groupby('Dominant_Topic')
    
        # Compute how much each topic contribtues in percentage
        for i, grp in sentence_topics_grouped:
            sentence_topics_sorted_df = pd.concat([sentence_topics_sorted_df, grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], axis=0)
    
        # Reset index
        sentence_topics_sorted_df.reset_index(drop=True, inplace=True)
    
        # Define columns in dataframe
        sentence_topics_sorted_df.columns = ['Topic_Num', "Topic_Perc_Contrib", "Keywords", "Representative Text"]
    
        # Save dataframe to output-folder
        output_path = os.path.join("..", "output", "topic_contributions.csv")
        sentence_topics_sorted_df.to_csv(output_path, index = False)

    
    def visualize_topics(self, processed_data, rolling_mean, lda_model, corpus):
        """
        This method creates visualizations using pyLDAvis and seaborn and saves these in the 'output' folder.
        """
        # Create viz object 
        viz = pyLDAvis.gensim.prepare(lda_model, 
                                      corpus, 
                                      dictionary = lda_model.id2word)
    
        # Save visualization as html-file
        out_path = os.path.join("..", "output", "lda_topics.html")
        pyLDAvis.save_html(viz, out_path)
    
        # Create list of values. The first entry is the topic, and the second entry is how much it contributes (percentage)
        values = list(lda_model.get_document_topics(corpus))
          
        # Split the values and keep only the values per topic
        split = []
        for entry in values:
            topic_prevelance = []
            for topic in entry:
                topic_prevelance.append(topic[1])
            split.append(topic_prevelance)
    
        # Create document-topic matrix
        matrix = pd.DataFrame(map(list,zip(*split)))
    
        # Create plot with rolling mean
        lineplot = sns.lineplot(data=matrix.T.rolling(rolling_mean).mean())
    
        # Set axes labels
        lineplot.set(xlabel="Tweet Chunks/batches", ylabel = "Topic Percentage Contribution")
    
        # Set title of plot
        lineplot.set_title('Topic Contribution Over Time', size = 20)
    
        # Set title of legend
        lineplot.legend(title='Topic', loc='upper right')
    
        # Get figure to be able to save
        fig = lineplot.get_figure()
    
        # Specifiy outputpath
        out_path = os.path.join("..", "output", "topics_over_time.jpg")
    
        # Save lineplot to output directory
        fig.savefig(out_path)
    
    
    def create_wordcloud(self, lda_model, optimal_n_topics):
        """
        This method takes the topics and creates word clouds to make the overview of the topics easier. 
        This method was inspired by the following article, but modified to fit to the this particular project:
        https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/ 
        """
        # Create list of colors from the matplotlib.colors 
        cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]

        # Define word cloud
        cloud = WordCloud(background_color='white',
                          width=2500,
                          height=2500,
                          max_words=10,
                          colormap='tab10',
                          color_func=lambda *args, **kwargs: cols[i],
                          prefer_horizontal=1.0)
        
        # LDA topics
        topics = lda_model.show_topics(num_topics = optimal_n_topics, formatted=False)
        
        # Define subplots. Since the number of subplots depends on the number of topics found I use an if-statement
        if (optimal_n_topics == 5):
            fig, axes = plt.subplots(1, 5, figsize=(20,20), sharex=True, sharey=True)
        
        if (optimal_n_topics == 10):
            fig, axes = plt.subplots(2, 5, figsize=(20,20), sharex=True, sharey=True)
            
        if (optimal_n_topics == 15):
                fig, axes = plt.subplots(3, 5, figsize=(20,20), sharex=True, sharey=True)
            
        # Generate a word cloud for each topic
        for i, ax in enumerate(axes.flatten()):
            fig.add_subplot(ax)
            topic_words = dict(topics[i][1])
            cloud.generate_from_frequencies(topic_words, max_font_size=300)
            plt.gca().imshow(cloud)
            plt.gca().set_title('Topic ' + str(i))
            plt.gca().axis('off')
            
            # Additional adjusting
            plt.subplots_adjust(wspace=0, hspace=0)
            plt.axis('off')
            plt.margins(x=0, y=0)
            plt.tight_layout()
        
        # Save word clouds to visualization folder
        output_path = os.path.join("..", "output", "topic_wordclouds.png")
        plt.savefig(output_path)
    
    
# Define behaviour when called from command line
if __name__=="__main__":
    main()